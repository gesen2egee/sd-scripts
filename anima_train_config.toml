# Anima LoRA training config for sd-scripts/anima_train_network.py
# Edit dataset/output_name as needed.

[model]
pretrained_model_name_or_path = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\diffusion_models\anima-preview.safetensors'
qwen3 = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\text_encoders\qwen_3_06b_base.safetensors'
vae = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\vae\qwen_image_vae.safetensors'

[dataset]
dataset_config = './anima_dataset_config.toml'

[output]
output_dir = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\loras'
logging_dir = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\loras\_logs_anima'
output_name = 'miss valentine anima 5'
save_model_as = 'safetensors'
save_precision = 'bf16'
save_every_n_steps = 250

[network]
network_module = 'lycoris.kohya'
network_args = ["algo=lokr", "factor=10", "full_matrix=True"]
network_train_unet_only = true
# Keep minimal LoRA targets first for stability.

[train]
learning_rate = 1
text_encoder_lr = 0.0
llm_adapter_lr = 0.0
optimizer_type = 'adv_optm.Prodigy_adv'
optimizer_args = ['d0=5e-5', 'weight_decay=0.1', 'cautious_wd=True','kourkoutas_beta=True', 'k_warmup_steps=300', "compiled_optimizer=False"]
lr_scheduler = 'cosine'
lr_warmup_steps = 300
train_batch_size = 1
max_grad_norm = 1.0
max_train_steps = 3000
max_data_loader_n_workers = 2
persistent_data_loader_workers = true

[memory]
mixed_precision = 'bf16'
transformer_dtype = 'bfloat16'
gradient_checkpointing = true
cache_latents = true
cache_latents_to_disk = true
cache_text_encoder_outputs = true
cache_text_encoder_outputs_to_disk = true

[anima]
timestep_sampling = 'sigmoid'
discrete_flow_shift = 1
random_noise_shift = 0
random_noise_multiplier = 0
random_noise_shift_random_strength = false
random_noise_multiplier_random_strength = false
random_noise_shift_decay = 1
random_noise_multiplier_decay = 1
qwen3_max_token_length = 512
t5_max_token_length = 512
