# Anima LoRA training config for sd-scripts/anima_train_network.py
# Edit dataset/output_name as needed.

[model]
pretrained_model_name_or_path = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\diffusion_models\anima-preview.safetensors'
qwen3 = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\text_encoders\qwen_3_06b_base.safetensors'
vae = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\vae\qwen_image_vae.safetensors'

[dataset]
dataset_config = './anima_dataset_config.toml'

[output]
output_dir = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\loras'
logging_dir = 'D:\SDXL\ComfyUI_windows_portable\ComfyUI\models\loras\_logs_anima'
output_name = 'miss valentine anima 2'
save_model_as = 'safetensors'
save_precision = 'bf16'
save_every_n_steps = 250

[network]
network_module = 'networks.lora_anima'
network_dim = 32
network_alpha = 32
network_train_unet_only = true
# Keep minimal LoRA targets first for stability.
network_args = []

[train]
learning_rate = 1
text_encoder_lr = 0.0
optimizer_type = 'adv_optm.Prodigy_adv'
optimizer_args = ['weight_decay=0.1', 'cautious_wd=True','kourkoutas_beta=True', 'k_warmup_steps=300']
lr_scheduler = 'cosine'
lr_warmup_steps = 0
train_batch_size = 2
max_grad_norm = 1.0
max_train_steps = 3000
max_data_loader_n_workers = 2
persistent_data_loader_workers = true

[memory]
mixed_precision = 'bf16'
transformer_dtype = 'bfloat16'
gradient_checkpointing = true
cache_latents = true
cache_latents_to_disk = true
cache_text_encoder_outputs = true
cache_text_encoder_outputs_to_disk = true

[anima]
timestep_sampling = 'sigmoid'
discrete_flow_shift = 1.0
sigmoid_scale = 1.0
qwen3_max_token_length = 512
t5_max_token_length = 512
